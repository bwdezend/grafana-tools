#!/usr/bin/python -tt
#
import datetime
import json
import logging
import logging.handlers
import optparse
import os
import codecs
import re
import shutil
import sys
import tempfile
import urllib
import urllib2
import git

def grafana_dashboard_backups(elastic_host, elastic_port, dest_dir, es_index_name, debug, quiet):
  '''Saves all changes to Grafana dashboards stored in Elasticsearch in a git repo
  
     Uses Elasticsearch API to get all saved dashboards into a git repo, the 
     repo contains one file per dashboard and is available in destination dir
     which is parameterized (defaults to /var/opt/grafana-dashboards-backups).'''
  
  #Set Logging to syslog
  try:
    logger = logging.getLogger(__name__)
    if debug:
      logger.setLevel(logging.DEBUG)
    elif quiet:
      logger.setLevel(logging.WARNING)
    else:
      logger.setLevel(logging.INFO)
  
    formatter = logging.Formatter('%(pathname)s: %(message)s')
  
    syslog_handler = logging.handlers.SysLogHandler(address = '/dev/log')
    syslog_handler.setFormatter(formatter)
    logger.addHandler(syslog_handler)
  except Exception:
    logging.info('Could not set syslog logging handler')


  if debug:
    urllib2.install_opener(urllib2.build_opener(urllib2.HTTPHandler(debuglevel=1)))


  def search(url, params):
    search_url = '%s?%s' %  (url, urllib.urlencode(params))
    try:
      response = urllib2.urlopen(search_url, timeout=5)
    except urllib2.URLError as e:
      if debug:
        logger.critical('Failed accessing: %s' % search_url)
      logger.critical(e)
      sys.exit(1)

    return json.load(response)

  def scan():
    return search(dashboards_url, {'search_type': 'scan', 'scroll': scroll_time})

  def scroll(scroll_id):
    url = '%s/%s' % (scroll_url, scroll_id)
    return search(url, {'scroll': scroll_time})
  
  def cam_case_convert(name):
    '''strips spaces and replace CamCasing.cam with cam_casing_cam'''
  
    s1 = re.sub('([^._])([A-Z][a-z]+)', r'\1_\2', name.replace(' ',''))
    s1 = s1.replace('.','_')
    return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()

  def json_dump(data,filename):
    dump_file = codecs.open(filename, 'w', 'utf-8')
    json.dump(data, dump_file, sort_keys = True, indent = 2, ensure_ascii=False)
    dump_file.close()
  
  # Conveniance vars
  es_type        = 'dashboard'
  es_url         = 'http://%s:%s' % (elastic_host, elastic_port)
  dashboards_url = '%s/%s/%s/_search' % (es_url, es_index_name, es_type)
  scroll_url     = '%s/_search/scroll' % es_url
  scroll_time    = '1m'
  work_tmp_dir   = tempfile.mkdtemp()
  utc_datetime   = datetime.datetime.utcnow()
  formatted_time = utc_datetime.strftime("%Y-%m-%d-%H%MZ")
  
  if debug:
    logger.info('Grabbing grafana dashboards from: %s' % dashboards_url)

  scroll_id = scan()['_scroll_id']

  # Init a git repo object, pointing at a repo with all the dashboards

  gitrepo_name = '%s' % (es_index_name)
  gitrepo      = os.path.join(dest_dir)
  repo         = git.Repo(gitrepo)
  
  commit_message = ''
  
  if not scroll_id:
    raise Exception('Failed to get scroll id')

  while 1:
    data      = scroll(scroll_id)
    hit_count = len(data['hits']['hits'])
    scroll_id = data['_scroll_id']
    if hit_count == 0:
      break

    try:
      for hit in data['hits']['hits']:
        source               = hit['_source']
        did                  = hit['_id']
        dashboard_definition = json.loads(source['dashboard'])
        dashboard_base_name  = cam_case_convert(did)

        dashboard_file_name  = '%s.json' % dashboard_base_name
        dashboard_file_path  = os.path.join(work_tmp_dir, dashboard_file_name)

        json_dump(dashboard_definition, dashboard_file_path)

        metadata_file_name = '%s.metadata.json' % dashboard_base_name
        metadata_file_path = os.path.join(work_tmp_dir, metadata_file_name)
        source['dashboard'] = 'stored in alternate file %s' % dashboard_file_name
        json_dump(hit, metadata_file_path)
        
        gitrepo_file_path  = os.path.join(gitrepo, dashboard_file_name)
        shutil.copyfile(dashboard_file_path, gitrepo_file_path)
        repo.index.add([dashboard_file_name])
        
        gitrepo_file_meta_path  = os.path.join(gitrepo, metadata_file_name)
        shutil.copyfile(metadata_file_path, gitrepo_file_meta_path)
        repo.index.add([metadata_file_name])

        logger.info('Added %s to the git repo' % did)

    except Exception as e:
      logging.critical(e)
      sys.exit(1)
  
  try:
    if repo.index.diff('HEAD'):
          
      commit_message = '%s\n' % formatted_time

      for diff in repo.index.diff('HEAD').iter_change_type('M'):
        logger.info(' M %s' % str(diff).splitlines()[0] )
        commit_message += ' %s was modified\n' % str(diff).splitlines()[0]
      for diff in repo.index.diff('HEAD').iter_change_type('D'):
        logger.info('%s was added' % diff)
        commit_message += ' %s was added\n' % str(diff).splitlines()[0]
      for diff in repo.index.diff('HEAD').iter_change_type('A'):
        logger.info('%s was deleted' % diff)
        commit_message += ' %s was deleted\n' % str(diff).splitlines()[0]
      for diff in repo.index.diff('HEAD').iter_change_type('R'):
        logger.info('%s was renamed' % diff)
        commit_message += ' %s was renamed\n' % str(diff).splitlines()[0]
      logger.info('Making a new git commit (%s)' % formatted_time)
      repo.index.commit(commit_message)
    else:
      logger.info('No changes detected. No commit made')

  except Exception as e:
    logging.critical('Failed to commit to git repo')
    logging.critical(e)
    sys.exit(1)
  
  # Clean up
  try:
    shutil.rmtree(work_tmp_dir)
  except Exception as e:
    logging.critical(e)
    sys.exit(1)

def main():
  parser = optparse.OptionParser()
  
  parser.add_option('-D', '--debug',
        action  = 'store_true',
        default = False,
        dest    = 'debug',
        help    = 'Debug output (very noisy)')

  parser.add_option('-Q', '--quiet',
        action  = 'store_true',
        default = False,
        dest    = 'quiet',
        help    = 'Supress output other than errors')
  
  parser.add_option('-e', '--grafana-elasticsearch-host',
    dest    = 'elastic_host',
    help    = 'The elastic search host FQDN used by grafana',
    metavar = 'ELASTIC_SEARCH_HOST')
  
  parser.add_option('-p', '--grafana-elasticsearch-port',
    default = '9200',
    dest    = 'elastic_port',
    help    = 'The elastic search port used by grafana',
    metavar = 'ELASTIC_SEARCH_PORT')
  
  parser.add_option('-t', '--dest-dir',
    default = '/var/opt/grafana-dashboards-backups',
    dest    = 'dest_dir',
    help    = 'The directory where dashboards git repo is located',
    metavar = 'DEST_DIR')

  parser.add_option('-i', '--index-name',
    default = 'grafana-dash',
    dest    = 'es_index_name',
    help    = 'the elasticsearch index that contains the dashboards',
    metavar = 'ELASTICSEARCH_INDEX')
  
  (options, args) = parser.parse_args()
  
  if not options.elastic_host:
    parser.error('An elastic search host is required')
  
  elastic_host = options.elastic_host
  elastic_port = options.elastic_port
  dest_dir = options.dest_dir
  debug = options.debug
  quiet = options.quiet
  
  grafana_dashboard_backups(elastic_host, elastic_port, dest_dir, options.es_index_name, debug, quiet)

if __name__ == '__main__':
  main()
